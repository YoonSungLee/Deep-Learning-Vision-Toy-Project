{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instance Segmentation을 이해함에 있어서 그 기저가 되는 Object Detection과 Segmentation을 이해하는 것은 필수이다. 이번 Report는 Object Detection의 개념과 작동원리에 대해서 명확하게 이해하는 것이 목적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/bGpdSjp.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object Detection을 이해하기 위해서는 그것의 기저인 Image Classification과 Localization을 이해해야한다. 맨 왼쪽 그림은 Image Classification으로 단순히 Image 내의 물체가 어떤 물체인지 분류해준다. 나아가 가운데 그림은 Image 내의 물체를 분류해주고 그 물체의 Bounding Box를 잡아준다. 이 때 지금까지 언급한 두 문제는 Image내의 하나의 물체만을 대상으로 한다. 만약 그 대상이 많아진다면 물체 각각에 대하여 분류해주고 Bounding Box를 잡아주어야 할 것이다. 이것이 바로 Object Detection이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/79QEXs2.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 언급한 문제 중 'Classification with Localization' 문제를 좀 더 살펴보자. 모델은 위와 같이 구성된다. 마지막 출력층을 보면 기존 Classification 문제를 해결하기 위한 softmax 분류 layer 뿐만 아니라 물체의 Bounding Box를 출력하는 층이 포함된다. 해당 층은 총 4개의 결과를 출력하는데, Bounding Box의 중심좌표에 해당하는 bx, by, Bounding Box의 높이와 너비에 해당하는 bh, bw가 그것이다. 특히 분류를 함에 있어서 Bounding Box안에 물체가 없을 경우가 있으므로 background에 해당하는 클래스를 하나 추가해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/CTwRVx9.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target label을 만드는 방법은 위와 같다. y 벡터는 물체의 존재 여부(물체가 있는지 나타내는 확률)를 표현하는 pc, Bounding Box의 정보를 표현하는 bx, by, bh, bw, 그리고 클래스 별 존재확률인 c1, c2, c3, ... 으로 구성된다. 특히 pc의 경우 물체가 있다면 1, 없다면 0의 값을 갖는데, 만약 물체가 없다면 나머지 원소들의 값이 무의믜하므로 신경쓰지 않는다. 따라서 Loss Function의 구성을 생각해보자면 다음과 같다. 먼저 물체가 존재하는 경우(y=1)는 y_hat과 y벡터의 L2 Loss를 통해 그 차이를 계산할 수 있다. 물체가 존재하지 않는 경우(y=0)는 pc의 값만 유의미하기 때문에 y_hat과 y 벡터의 pc에 해당하는 원소만 제곱 오차를 사용하여 그 차이를 계산한다. 물론 이는 가장 간편한 방법이고, 설계에 따라서 pc부분을 로지스틱 회귀 손실을 사용할 수도 있고, c1, c2, c3, ...부분을 로그 우도 손실을 사용할 수  도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landmark Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/buozZbl.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 다음 살펴볼 내용은 Landmark Detection(특징점 검출)이다. 흔히 Face Recognition 문제나 Pose Estimation 문제가 이에 해당한다. 이는 Object Localization과는 달리 얼굴 인식에 중요한 특징점 또는 자세 추정에 중요한 특징점 등 문제를 해결하기에 중요하다고 생각하는 특징점들의 좌표를 예측하는 문제이다. 따라서 y 벡터는 물체의 존재 여부(물체가 있는지 나타내는 확률)를 표현하는 pc, 특징점들의 좌표를 표현하는 l1x, l1y, l2x, l2y, ...로 구성되어있다. 예를 들어 위의 가운데 이미지는 Face Recognition 문제에 해당하는데, 얼굴의 존재여부를 표현하는 원소, 그리고 얼굴인식에 중요하다고 생각하는 64개 특징점의 좌표를 표현하는 원소로 이루어져 총 129개의 원소를 가지는 y 벡터가 필요할 것이다. 특히 중요한 점은 특징점은 다른 이미지에 대해서도 항상 동일해야 한다는 점이다. 예를 들어 특징점 1은 항상 왼쪽 눈의 눈꼬리가 되어야 하고, 특징점 2는 항상 오른쪽 눈의 눈꼬리가 되어야 한다. 이렇듯 레이블 순서는 다른 이미지에 대해서도 항상 동일하게 구성되어 있어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/3mVAGcC.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 Object Detection에 대해서 살펴보자. 이는 물체의 위치를 탐지하여 BBox를 그리고 물체를 분류하는 문제라고 앞에서 언급했다. 기본적인 작동방식은 위의 그림과 같다. 먼저 전체 Image에서 자동차가 있을 만한 곳의 BBox를 모두 잡는다. 물론 자동차를 잡지 못한 BBox 또한 존재할 것이다. 이 모든 BBox를 ConvNet에 집어넣어 자동차인지를 분류하면 된다. 요약하자면, 물체가 있을만한 곳의 BBox를 모두 잡아내고, 그것들을 모두 ConvNet에 집어넣어 물체의 존재여부를 분류한다. 마찬가지로 ConvNet을 훈련시키기 위해서는 배경을 제외한 자동차만 존재하는 BBox와 그 레이블 데이터를 준비해야 할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/D3aQAek.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그렇다면 자동차가 있을만한 곳의 BBox는 어떻게 잡는걸까? 이 방법 중의 하나로 'Sliding windows detection'이 있다. 미리 정해둔 크기의 window를 Image상에서 일정간격마다 적용시켜 수많은 BBox를 만드는 방법이다. 이후 추출한 BBox들을 ConvNet에 집어넣으면 될 것이다. 간단해보이는 방법이지만 단점이 존재한다. 이 방법은 이미지의 수많은 영역을 모두 잘라내야 하고 합성곱 신경망을 통해 이것들을 각각 계산해야 하기 때문에 시간과 비용의 문제가 발생한다. 매우 정밀한 입도 또는 이동간격을 사용한다면 모든 작은 영역을 합성곱 신경망에 통과시켜야 하기 때문에 매우 높은 계산 비용이 필요하다. 반대로 매우 큰 슬라이드 간격을 사용한다면 합성곱 신경망을 통과시켜야 하는 window의 수는 줄어들지만 물체를 제대로 탐지하지 못해 성능이 저하되는 문제가 발생한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Implementation Sliding Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 제시한 Sliding Windows 방법은  Window 각각을 ConvNet에 입력해야 하기 때문에 시간이 오래 걸린다는 단점을 확인했다. 하지만 이는 Convolutional Implementation을 통해 그 문제를 해결할 수 있는데, 살펴보도록 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/Ubwnwb9.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 FC가 어떻게 Convolutional Layer로 바뀔 수 있는지 살펴보자. 위의 그림의 첫 번째는 FC Layers를 의미하고, 두 번째는 Convolutional Layers를 의미한다. FC Layers에서는 5x5x16의 feature map을 단순히 FC를 이용하여 400개의 출력을 만들어낸다. 반면에 이를 400 filter의 5x5 conv를 이용한다면 1x1x400의 출력을 만들어내어 마치 FC와 같은 기능을 하는 역할을 한다. 핵심은 1x1 conv라고 할 수 있겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/BEz4j2G.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자 그럼 이 방법을 이용해서 무슨 이점을 얻을 수 있는지 살펴보자. 위의 그림의 첫 번째 내용은 14x14x3 Image에 대하여 적용한 내용으로써 이전 슬라이드와 같다. 두 번째, 세 번째의 Image에 14x14 사이즈의 Sliding Window를 적용한다고 가정해보자.<br>\n",
    "두 번째 내용 기준으로, 만약 우리가  Convolutional Implementation을 사용하지 않는다면 총 4개의 Window를 ConvNet에 적용시킬것이다. 예시이기 때문에 작은 사이즈 Image에 적용했지만 만약 더 큰 사이즈의 Image라면 더 많은 수의 Window를 ConvNet에 적용시켜야 하기 때문에 시간이 오래 걸린다. 또한 각 Window는 겹치는 부분이 많아서 ConvNet이 많은 반복 계산을 수행한다는 비효율이 발생한다.<br>\n",
    "Convolutional Implementation을 사용한다면 어떻게 될까? 16x16x3 Image에 첫 번째 내용과 같은 구조의 모델을 적용시킨다면 총 2x2x4의 출력을 얻을 수 있는데, 이는 곧 4개의 Window를 ConvNet에 입력시켜야하는 과정을 한 번에 수행할 수 있다는 것을 보여준다. 출력값의 (1,1)은 첫 번째 Window를 적용한 값을 나타내고, 출력값의 (1,2)는 두 번째 Window를 적용한 값을 나타낸다고 볼 수 있겠다. 즉, 연산과정에서 겹치는 반복계산을 공유하는 이점을 가질 수 있다. 그리고 각 위치에 대해서 4개의 값을 갖게 되는데, 이는 마치 Softmax함수를 수행한 것과 같은 효과를 얻을 수 있다. 즉, 각각의 Window가 어느 클래스를 나타내는지 분류하는 문제와 같다.<br>\n",
    "마찬가지로 세 번째 내용을 살펴보면 Sliding Window를 통해 64개의 filter map을 ConvNet에 적용시키는 과정을 단 한 번에 수행하는 것을 보여주고 있다고 할 수 있겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/4E7ANOT.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정리하자면, Convolutional Implementation을 이용하여 각각의 Window를 ConvNet에 집어넣는 과정을 단 한번에 수행할 수 있는 모델을 만들 수 있다. 하지만 이 알고리즘은 여전히 문제를 가지고 있는데, BBox의 위치가 정확하지 않을 수 있다는 것이 그것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intersection Over Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/5r1L3Sr.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번엔 Object Detection이 잘 작동하는지 확인하는 방법, 즉 성능평가방법에 대해서 알아보자. 그 명칭을 먼저 말하자면 'IoU(Intersection over Union)'이라고 한다. 명칭이 꽤 직관적으로 알 수 있도록 지었다는 느낌이 든다. 위의 그림을 예로 들면, 물체의 실제 BBox는 빨간색 박스이고 예측 BBox는 보라색 박스이다. Union은 두 박스의 모든 영역으로써 초록색으로 빗금친 영역을 의미하고, Intersection은 두 박스의 공통적인 영역으로써 노란색으로 빗금친 영역을 의미한다. 이 두 영역을 나눠주기만 하면 IoU를 구할 수 있다. 만약 예측 BBox가 실제 BBox를 정확하게 예측했다면 이 값은 1이 될 것이고, 공통부분 없이 완전히 틀리게 예측했다면 이 값은 0이 될 것이다. 관례적으로 컴퓨터 비전분야에서는 이 값이 0.5 이상만 되면 성능이 괜찮다라고 평가하여 임계값을 0.5로 설정하고는 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-max Suppression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/kC7USCe.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 배운 Object Detection 방식은 단점이 하나 있다. 초기에 지정해준 수많은 BBox가 학습을 거쳐 물체를 가리킬텐데, 중복된 BBox들이 많을것이라는 점이다. 예를 들어 위의 왼쪽 그림처럼 19x19개의 그리드를 ConvNet에 집어넣는다고 해보자. 자동차 주변의 몇몇 그리드는 예측한 BBox가 자동차라고 예측하고 검출할것이다. 이러한 몇몇 그리드는 결국 같은 자동차를 검출하는 셈이 된다. 이런 중복된 BBox에 대해서 제거를 해 주는 것이 명확한 결과를 준다. 이를 해결하는 방법이 Non-max Supression이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/M4H8vVe.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그림으로 예를 들어보자. 왼쪽 자동차를 0.8로 예측한 BBox와 0.7로 예측한 BBox는 결국 중복이므로 하나를 제거해야하고, 오른쪽 자동차를 각각 0.6, 0.7, 0.9로 예측한 BBox들도 중복이므로 하나만 남기고 제거해야한다. 이를 제거할때는 BBox 내에 물체가 있다고 예측하는 확률인 Pc가 가장 높은 것을 남기고 제거하는것이 상식적이다. 즉, 이 경우에 NMS를 적용하고 나면 0.8로 예측한 BBox와 0.9로 예측한 BBox가 남겨질것이라고 예상할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/JOHkgtc.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그럼 Non-max Supression 작동방식에 대해 살펴보자. 위 그림에서는 하나의 물체를 검출하는 상황이라고 가정하자.먼저 모든 BBox에 대하여 Pc가 0.6 이하인 것들은 다 버린다. 이는 물체를 검출했다고 보기 어렵기 때문이다. 이후 남아있는 박스들에 대해서 NMS를 수행한다. 특정 클래스(위 그림에서는 하나의 클래스만 존재)를 기준으로 가장 높은 Pc값을 가지고 있는 BBox를 선택한다. 이후 해당 BBox와 남아있는 BBox를 비교하여 IoU가 0.5 이상인 것들은 중복이라고 판단하고 버린다. 마찬가지로 남이있는 박스들에 대해서 또 다시 가장 높은 Pc값을 가지고 있는 BBox를 선택하는 과정을 반복한다. 만약 보행자, 자동차, 오토바이 같은 세 개의 물체를 검출하려 한다면 결과 벡터는 추가적인 세 개의 요소를 가질 것이고 각각의 결과 클래스에 대해 독립적으로 세 번의 NMS를 해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDoNext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Question 해결/ 코드이해\n",
    "* https://dyndy.tistory.com/275\n",
    "* https://hwiyong.tistory.com/119\n",
    "* https://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question)<br>\n",
    "NMS 과정에서 하나의 BBox를 선택하고나면 해당 BBox와 IoU가 높은 BBox들은 중복이라고 판단하고 제거하는 것으로 알고있습니다. 그렇다면 동일한 클래스가 뭉쳐있거나 겹쳐있다면 중복인 줄 알고 BBox를 지워버리지 않나요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(boxes, probs, threshold):\n",
    "    \"\"\"Non-Maximum supression.\n",
    "    Args:\n",
    "      boxes: array of [cx, cy, w, h] (center format)\n",
    "      probs: array of probabilities\n",
    "      threshold: two boxes are considered overlapping if their IOU is largher than\n",
    "          this threshold\n",
    "      form: 'center' or 'diagonal'\n",
    "    Returns:\n",
    "      keep: array of True or False.\n",
    "    \"\"\"\n",
    " \n",
    "    order = probs.argsort()[::-1]\n",
    "    keep = [True]*len(order)\n",
    " \n",
    "    for i in range(len(order)-1):\n",
    "      ovps = batch_iou(boxes[order[i+1:]], boxes[order[i]])\n",
    "      for j, ov in enumerate(ovps):\n",
    "        if ov > threshold:\n",
    "          keep[order[j+i+1]] = False\n",
    "    return keep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deeplearning.ai\n",
    "* https://www.deeplearning.ai/\n",
    "* https://www.youtube.com/channel/UCcIXc5mJsHVYTZR1maL5l9w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
