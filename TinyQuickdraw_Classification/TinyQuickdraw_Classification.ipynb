{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation\n데이터셋을 생성하는 코드입니다. 이 부분은 수정하지 않으셔도 됩니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport json\nimport shutil\nimport cv2\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_dir = \"/kaggle/input/tinyquickdraw/quickdraw_simplified\"\nclass_files = os.listdir(img_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def strokes2img(strokes):\n    \"\"\"Strokes 형식의 데이터를 28x28 크기의 grayscale 이미지로 변환\"\"\"\n    img = np.zeros((28, 28, 1))\n    for stroke in strokes:\n        pts = np.round(np.array(stroke).transpose().reshape(-1, 1, 2) * (28 / 256)).astype(int)\n        img = cv2.polylines(img, [pts], isClosed=False, color=(255, 255, 255))\n    img = np.array(255 - img)\n    img = cv2.resize(img, dsize=(28, 28), interpolation=cv2.INTER_LINEAR)\n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_lines(data_path):\n    \"\"\"데이터 파일의 line 수를 리턴\"\"\"\n    with open(data_path, \"r\") as d:\n        return sum(1 for line in d)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 지정된 폴더와 하위 디렉토리 폴더, 파일을 모두 삭제\n\ndef init_dirs():\n    \"\"\"디렉토리 초기화\"\"\"\n    if os.path.exists(\"train\"):\n        shutil.rmtree(\"train\")\n    if os.path.exists(\"val\"):\n        shutil.rmtree(\"val\")\n    if os.path.exists(\"test\"):\n        shutil.rmtree(\"test\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ndjson 형식의 파일을 이미지 파일로 바꿔 저장합니다.\n# 총 345개의 클래스가 있지만, 용량 문제로 인해 200개의 클래스만 사용하며,\n# 원래 해당 클래스에 속한 이미지 수의 1/100을 임의로 표집하여 사용합니다.\n\nnum_classes = 200\nsample_scale = 100\n\nnp.random.seed(0)\n\nprint(\"This process may take a while...\")\n\nstart_time = time.time()\n\ninit_dirs()\n\nfor class_idx, class_file in enumerate(class_files):\n    \n    if class_idx == num_classes:\n        break\n    \n    class_name = class_file.split(\".\")[0]\n\n    if not os.path.exists(f\"train/{class_name}\"):\n        os.makedirs(f\"train/{class_name}\")\n    \n    if not os.path.exists(f\"val/{class_name}\"):\n        os.makedirs(f\"val/{class_name}\")\n\n    if not os.path.exists(f\"test/{class_name}\"):\n        os.makedirs(f\"test/{class_name}\")\n\n    file_path = os.path.join(img_dir, class_file)\n    num_images = get_lines(file_path)\n    num_samples = num_images // sample_scale\n    sample_indices = sorted(np.random.choice(num_images, num_samples, replace=False).tolist())\n    \n    print(f\"Processing class {class_idx + 1:03d}/{num_classes}, picking up {num_samples:06d} images. Time elapsed: {(time.time() - start_time)/60:.2f} mins.\", end=\"\\r\")\n    \n    with open(file_path, \"r\") as d:\n        for idx, line in enumerate(d):\n            if idx == sample_indices[0]:\n                strokes = json.loads(line)[\"drawing\"]\n                img = strokes2img(strokes)\n                \n                r = np.random.random()\n                if r < 0.8:\n                    cv2.imwrite(f\"train/{class_name}/{idx}.png\", img)\n                elif r < 0.9:\n                    cv2.imwrite(f\"val/{class_name}/{idx}.png\", img)\n                else:\n                    cv2.imwrite(f\"test/{class_name}/{idx}.png\", img)\n                \n                sample_indices.pop(0)\n                if len(sample_indices) == 0:\n                    break\n            else:\n                pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_csv_file(division):\n    \"\"\"저장된 이미지를 기반으로 데이터 로더를 위한 csv 파일을 작성합니다.\"\"\"\n    print(f\"Writing csv data file for {division}...\")\n    i = 0\n    with open(f\"{division}.csv\", \"w\") as csv_file:\n        csv_file.write(\"path,cls\\n\")\n        base_dir = division\n        classes = os.listdir(base_dir)\n        for cls in classes:\n            imgs = os.listdir(os.path.join(base_dir, cls))\n            for img in imgs:\n                path = os.path.join(base_dir, cls, img)\n                csv_file.write(f\"{path},{cls}\\n\")\n                i += 1\n        print(f\"Wrote {division}.csv file with {i} lines.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"make_csv_file(\"train\")\nmake_csv_file(\"val\")\nmake_csv_file(\"test\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Class Ratio","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe = pd.read_csv('train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_list = list(set(dataframe['cls']))\nall_classes = list(dataframe['cls'])\nclass_count_dict = dict()\n\nprint('%s %10s'%('class_name'.ljust(20), 'count'))\nprint('-'*33)\nfor cls in class_list:\n    counts = all_classes.count(cls)\n    print('%s %10d'%(cls.ljust(20), counts))\n    class_count_dict[cls] = counts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# There is a class imbalance problem\n\ndef f1(x):\n    return class_count_dict[x]\n\nkey_max = max(class_count_dict.keys(), key=f1)\nkey_min = min(class_count_dict.keys(), key=f1)\n\nprint('Max : %s --> %d'%(key_max, class_count_dict[key_max]))\nprint('Min : %s --> %d'%(key_min, class_count_dict[key_min]))\nprint('Difference :', class_count_dict[key_max] - class_count_dict[key_min])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Train\n자유롭게 수정해주세요.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.autograd import Variable\nimport torchvision\nfrom torchvision import transforms\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for Baseline Model\nclass CustomDataset(Dataset):\n    \n    def __init__(self, csv_path=\"train.csv\", transform=None):\n        data = pd.read_csv(csv_path)\n        self.paths = data[\"path\"].values\n        self.classes = data[\"cls\"].values\n        self.cls_to_idx = {cls: idx for idx, cls in enumerate(set(self.classes))}\n        self.transform = transforms.Compose([\n            transforms.ToTensor()\n        ])\n    \n    def __getitem__(self, idx):\n        path = self.paths[idx]\n        img = self.transform(Image.open(path))\n        cls_idx = self.cls_to_idx[self.classes[idx]]\n        return path, img, cls_idx\n    \n    def __len__(self):\n        return len(self.paths)\n    \n    \n\n# for VGG16_bn and ResNet50 Model\n# class CustomDataset(Dataset):\n    \n#     def __init__(self, csv_path=\"train.csv\", transform=None):\n#         data = pd.read_csv(csv_path)\n#         self.paths = data[\"path\"].values\n#         self.classes = data[\"cls\"].values\n#         self.cls_to_idx = {cls: idx for idx, cls in enumerate(set(self.classes))}\n#         self.transform = transforms.Compose([\n#             transforms.Grayscale(num_output_channels=3),\n#             transforms.Resize(224),\n#             transforms.ToTensor()\n#         ])\n\n    \n#     def __getitem__(self, idx):\n#         path = self.paths[idx]\n#         img = self.transform(Image.open(path))\n#         cls_idx = self.cls_to_idx[self.classes[idx]]\n#         return path, img, cls_idx\n    \n#     def __len__(self):\n#         return len(self.paths)\n\n\n\n# for TS-CNN and AI_Inno_CNN Model\n# class CustomDataset(Dataset):\n    \n#     def __init__(self, csv_path=\"train.csv\", transform=None):\n#         data = pd.read_csv(csv_path)\n#         self.paths = data[\"path\"].values\n#         self.classes = data[\"cls\"].values\n#         self.cls_to_idx = {cls: idx for idx, cls in enumerate(set(self.classes))}\n#         self.transform = transforms.Compose([\n#             transforms.Resize(32),\n#             transforms.ToTensor()\n#         ])\n    \n#     def __getitem__(self, idx):\n#         path = self.paths[idx]\n#         img = self.transform(Image.open(path))\n#         cls_idx = self.cls_to_idx[self.classes[idx]]\n#         return path, img, cls_idx\n    \n#     def __len__(self):\n#         return len(self.paths)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = CustomDataset(csv_path=\"train.csv\")\ntrain_data_loader = DataLoader(train_dataset, batch_size=64, num_workers=0, shuffle=True)\n\nval_dataset = CustomDataset(csv_path=\"val.csv\")\nval_data_loader = DataLoader(val_dataset, batch_size=128, num_workers=0, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, criterion, optimizer, epochs, ES_patience=7):\n    # cuda check\n    model = model.cuda()\n    model.train()\n    losses = []\n    acc_list = []\n    ES_patience = ES_patience\n    ES_counter = 0\n    ES_best_val = float('inf')\n    \n    \n    for epoch in range(epochs):\n        print(f\"Epoch {epoch + 1} start\")\n        for batch_idx, (path, img, cls_idx) in enumerate(train_data_loader):\n            cls_scores = model(img.cuda())\n            cost = criterion(cls_scores, cls_idx.cuda())\n            optimizer.zero_grad()\n            cost.backward()\n            optimizer.step()\n            if (batch_idx + 1) % 100 == 0 or (batch_idx + 1) == len(train_data_loader):\n                print(f\"Epoch: {epoch + 1:03d}/{epochs:03d} | Batch: {batch_idx + 1:04d}/{len(train_data_loader):04d} | Cost: {cost.item():.4f}\")\n                losses.append(cost.item())\n        acc, val_cost = evaluate(model, val_data_loader, criterion)\n        acc_list.append(acc)\n        \n        # Early Stopping\n        if val_cost < ES_best_val:\n            ES_counter = 0\n            ES_best_val = val_cost\n        else:\n            ES_counter += 1\n        if ES_counter == ES_patience:\n            print('Early Stopping')\n            break\n        \n    plt.figure(figsize=(12,6))\n\n    plt.subplot(1,2,1)\n    plt.plot(losses)\n    plt.tick_params(\n        axis='x',          # changes apply to the x-axis\n        which='both',      # both major and minor ticks are affected\n        bottom=False,      # ticks along the bottom Edge are off\n        top=False,         # ticks along the top Edge are off\n        labelbottom=False) # labels along the bottom Edge are off\n    plt.title('loss curve')\n    plt.grid()\n\n    plt.subplot(1,2,2)\n    plt.plot(acc_list, color='red')\n    plt.ylim([0,1])\n    plt.xlabel('epoch')\n    plt.title('accuracy for validation dataset')\n    plt.grid()\n    print('Best val accuracy of this model is %.4f in %d epoch'%(max(acc_list), acc_list.index(max(acc_list))+1))\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(model, data_loader, criterion):\n    \"\"\"모델을 통한 prediction 및 accuracy 계산\"\"\"\n    model.eval()\n    with torch.no_grad():\n        corrects_list = list()\n        for path, img, cls_idx in val_data_loader:\n            cls_scores = model(img.cuda())\n            cost = criterion(cls_scores, cls_idx.cuda())\n            cls_preds = torch.argmax(cls_scores, dim=1).cpu().numpy()\n            corrects_list.extend(list(cls_preds == cls_idx.numpy()))\n        accuracy = np.mean(corrects_list)\n        print(f\"Accuracy: {accuracy:.4f}\")\n    return accuracy, cost.item()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Baseline Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class Baseline(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        self.num_classes = 200\n        self.conv_layer = nn.Sequential(\n            nn.Conv2d(1, 6, 5, 1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(6, 16, 5, 1),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        \n        self.fc_layer = nn.Sequential(\n            nn.Linear(16*4*4, 256),\n            nn.ReLU(),\n            nn.Linear(256, self.num_classes),\n        )\n        \n    def forward(self, x):\n        x = self.conv_layer(x)\n        x = x.view(x.shape[0], -1)\n        x = self.fc_layer(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Baseline()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\ncriterion = nn.CrossEntropyLoss()\nepochs = 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# VGG16_bn(pretrained)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vgg16_bn = torchvision.models.vgg16_bn(pretrained=True).cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for param in vgg16_bn.parameters():\n    param.requires_grad = False\n    \nnum_ftrs = vgg16_bn.classifier[6].in_features\nvgg16_bn.classifier[6] = nn.Linear(num_ftrs, num_classes)\n\nfor param in vgg16_bn.classifier.parameters():\n    param.requires_grad = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = torch.optim.Adam(vgg16_bn.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\nepochs = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vgg16_bn = train(vgg16_bn, criterion, optimizer, epochs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Resnet50(pretrained)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"resnet50 = torchvision.models.resnet50(pretrained=True).cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for param in resnet50.parameters():\n    param.requires_grad = False\n\nnum_ftrs = resnet50.fc.in_features\nresnet50.fc = nn.Linear(num_ftrs, num_classes)\n\nresnet50.fc.weight.requires_grad = True    # double check","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = torch.optim.Adam(resnet50.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\nepochs = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resnet50 = train(resnet50, criterion, optimizer, epochs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Like_VGG","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class Like_VGG(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        self.num_classes = 200\n        self.conv_layer = nn.Sequential(\n            # (B, 1, 28, 28)\n            nn.Conv2d(1, 32, 3, 1),\n            # (B, 32, 26, 26)\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, 3, 1),\n            # (B, 32, 24, 24)\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            # (B, 32, 12, 12)\n            nn.Conv2d(32, 64, 3, 1),\n            # (B, 64, 10, 10)\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, 3, 1),\n            # (B, 64, 8, 8)\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            # (B, 64, 4, 4)\n        )\n        \n        self.fc_layer = nn.Sequential(\n            nn.Linear(64*4*4, 64*4*4),\n            nn.ReLU(),\n            nn.Linear(64*4*4, self.num_classes),\n        )\n        \n        # weights initializer\n        for m in self.modules():\n            if isinstance(m, (nn.Conv2d, nn.Linear)):\n               nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n               nn.init.constant_(m.weight, 1)\n               nn.init.constant_(m.bias, 0)\n        \n    def forward(self, x):\n        x = self.conv_layer(x)\n        x = x.view(x.shape[0], -1)\n        x = self.fc_layer(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Like_VGG = Like_VGG()\noptimizer = torch.optim.Adam(Like_VGG.parameters(), lr=0.001, )\ncriterion = nn.CrossEntropyLoss()\nepochs = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Like_VGG = train(Like_VGG, criterion, optimizer, epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Like_VGG = Like_VGG()\noptimizer = torch.optim.RMSprop(Like_VGG.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\nepochs = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Like_VGG = train(Like_VGG, criterion, optimizer, epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Like_VGG = Like_VGG()\noptimizer = torch.optim.AdamW(Like_VGG.parameters(), lr=0.001, weight_decay=0.01)\ncriterion = nn.CrossEntropyLoss()\nepochs = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Like_VGG = train(Like_VGG, criterion, optimizer, epochs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Like_VGG with Focal Loss","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"[Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002)<br>\n[code reference link](https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/65938)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class FocalLoss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=2, logits=True, reduce=True):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.logits = logits\n        self.reduce = reduce\n\n    def forward(self, inputs, targets):\n        y = torch.zeros(targets.shape[0], num_classes)\n        y[range(y.shape[0]), targets]=1\n        targets = y.cuda()\n        if self.logits:\n            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n        else:\n            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n        pt = torch.exp(-BCE_loss)\n        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n\n        if self.reduce:\n            return torch.mean(F_loss)    # to check for changes: mean --> sum\n        else:\n            return F_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Like_VGG_FL = Like_VGG()\noptimizer = torch.optim.Adam(Like_VGG_FL.parameters(), lr=0.001)\ncriterion = FocalLoss()\n# criterion = nn.CrossEntropyLoss()\nepochs = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Like_VGG_FL = train(Like_VGG_FL, criterion, optimizer, epochs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# AI Inno CNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class AI_Inno_CNN(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        self.num_classes = 200\n        self.conv_layer = nn.Sequential(\n            # (B, 1, 32, 32)\n            nn.Conv2d(1, 32, 3, 1),\n            # (B, 32, 30, 30)\n            nn.ReLU(),\n            nn.Conv2d(32, 64, 3, 1),\n            # (B, 64, 28, 28)\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.MaxPool2d(2),\n            # (B, 64, 14, 14)\n            \n            nn.Conv2d(64, 64, 3, 1),\n            # (B, 64, 12, 12)\n            nn.ReLU(),\n            nn.Conv2d(64, 128, 3, 1),\n            # (B, 128, 10, 10)\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n            nn.MaxPool2d(2),\n            # (B, 128, 5, 5)\n            \n            nn.Conv2d(128, 256, 3, 1),\n            # (B, 256, 3, 3)\n            nn.ReLU(),\n            nn.BatchNorm2d(256)\n        )\n        \n        self.fc_layer = nn.Sequential(\n            # (B, 256)\n            nn.Linear(256, self.num_classes),\n            # (B, 200)\n            nn.ReLU(),\n            nn.Linear(self.num_classes, self.num_classes),\n            # (B, 200)\n        )\n        \n        # weights initializer\n        for m in self.modules():\n            if isinstance(m, (nn.Conv2d, nn.Linear)):\n               nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n               nn.init.constant_(m.weight, 1)\n               nn.init.constant_(m.bias, 0)\n        \n    def forward(self, x):\n        x = self.conv_layer(x)\n        x = F.adaptive_avg_pool2d(x, (1,1))\n        x = x.view(x.shape[0], -1)\n        x = self.fc_layer(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AI_Inno_CNN = AI_Inno_CNN()\noptimizer = torch.optim.Adam(AI_Inno_CNN.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\nepochs = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AI_Inno_CNN = train(AI_Inno_CNN, criterion, optimizer, epochs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TS-CNN","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"HAND DRAWN SKETCH CLASSIFICATION USING CONVOLUTIONAL NEURAL NETWORKS [link](https://www.semanticscholar.org/paper/HAND-DRAWN-SKETCH-CLASSIFICATION-USING-NEURAL-Atabay/e688a6535dbdd6ce6928bc4eb2978f39628e5302)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class TS_CNN(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        self.num_classes = 200\n            # (B, 1, 32, 32)\n        self.L1 = nn.Sequential(\n            nn.Conv2d(1, 96, 5, 1),\n            # (B, 96, 28, 28)\n            nn.ReLU(),\n            nn.MaxPool2d(3, stride=2),\n            # (B, 96, 14, 14)\n        )\n        self.L2 = nn.Sequential(\n            nn.Conv2d(96, 192, 5, 1),\n            # (B, 192, 10, 10)\n            nn.ReLU(),\n            nn.MaxPool2d(3, stride=2),\n            # (B, 192, 4, 4)\n        )\n        self.L3 = nn.Sequential(\n            nn.Conv2d(192, 192, 3, 1),\n            # (B, 192, 2, 2)\n            nn.ReLU()\n        )\n        self.L4 = nn.Sequential(\n            nn.Conv2d(192, self.num_classes, 1, 1),\n            # (B, 200, 2, 2)\n            nn.ReLU(),\n            nn.Conv2d(self.num_classes, self.num_classes, 1, 1)\n            \n        )\n        # weights initializer\n        for m in self.modules():\n            if isinstance(m, (nn.Conv2d, nn.Linear)):\n               nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n               nn.init.constant_(m.weight, 1)\n               nn.init.constant_(m.bias, 0)\n        \n        \n    def forward(self, x):\n        x = self.L1(x)\n        x = self.L2(x)\n        x = self.L3(x)\n        x = self.L4(x)\n        x = x.view(x.shape[0], -1)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TS_CNN = TS_CNN()\noptimizer = torch.optim.SGD(TS_CNN.parameters(), lr=0.001, momentum=0.9,\n                           weight_decay=0.0005)\ncriterion = nn.CrossEntropyLoss()\nepochs = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TS_CNN = train(TS_CNN, criterion, optimizer, epochs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation\n최종 스코어입니다. 이 정확도를 최대한 올려주세요!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = CustomDataset(csv_path=\"test.csv\")\ntest_data_loader = DataLoader(test_dataset, batch_size=128, num_workers=0, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate(model, test_data_loader)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}